# -*- coding: utf-8 -*-
"""Multi-Agent_System_With_LangGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sb0A1YwKNdCTH56kzhq_oPkTHsMuOOOv

# Multi Agent System with LangGraph

**Problem Statement:**

Traditional customer support in the insurance industry is slow, fragmented, and inefficient. Customers often face:

Long wait times and multiple handoffs
Repetitive data requests (‚ÄúCan you confirm your policy number again?‚Äù)
Generic chatbot responses that lack context or accuracy
Difficulty accessing personalized policy, billing, or claim information in real-time
Even with automation, most systems rely on single-intent chatbots that cannot handle the complex, multi-domain nature of insurance inquiries. This results in frustrated customers, overloaded support teams, and missed efficiency opportunities.

**Solution Approach:**

We built an AI-powered multi-agent insurance support system that mimics a team of human specialists using LangGraph orchestration and RAG (Retrieval-Augmented Generation).

A Supervisor Agent analyzes user intent and routes each query to specialized agents for policy, billing, claims, or general help.
These agents access real data via SQLite and ChromaDB, ensuring factual, context-aware responses.

By managing conversation state, routing logic, and data retrieval intelligently, the system delivers accurate, personalized, and scalable customer support ‚Äî while seamlessly escalating complex cases to humans when needed.
"""

!pip install langgraph langchain-openai langchain-community duckduckgo-search beautifulsoup4 requests python-dotenv tiktoken pandas chromadb sentence_transformers scikit-learn seaborn matplotlib datasets arize-phoenix-otel openinference-instrumentation-openai

pip install langchain-groq

import pandas as pd
import numpy as np
import json
import re
from typing import List, Dict, Any, Tuple
from openai import OpenAI
import time
from dotenv import load_dotenv
import openai
import os
from datasets import load_dataset
import chromadb
from datetime import datetime, timedelta
import random
import pandas as pd
import sqlite3
from langchain_groq import ChatGroq
# from opentelemetry.trace.status import Status, StatusCode
# from opentelemetry.trace import get_current_span
# from phoenix.otel import register

# Load the dataset from Hugging Face
ds = load_dataset("deccan-ai/insuranceQA-v2")

# Combine all splits into a single DataFrame
df = pd.concat([split.to_pandas() for split in ds.values()], ignore_index=True)
df["combined"] = "Question: " + df["input"] + " \n Answer:  " + df["output"]
# Inspect
print(df.shape)
df.head()

# Setting up the Chromadb
chroma_client = chromadb.PersistentClient(path="./chroma_db")
# Create or get the collection
collection = chroma_client.get_or_create_collection(name="insurance_FAQ_collection")

# Collection 1 for insurance Q&A Dataset
from tqdm import tqdm
df = df.sample(500, random_state=42).reset_index(drop=True)  # For testing, use a smaller subset
# Add data to collection
# here the chroma db will use default embeddings (sentence transformers)
# Split into batches of <= 5000
batch_size = 100

for i in tqdm(range(0, len(df), batch_size)):
    batch_df = df.iloc[i:i+batch_size]
    collection.add(
        documents=batch_df["combined"].tolist(),
        metadatas=[{"question": q, "answer": a} for q, a in zip(batch_df["input"], batch_df["output"])],
        ids=batch_df.index.astype(str).tolist()
    )

## Testing the retrieval
query = "What does life insurance cover?"
collection = chroma_client.get_collection(name="insurance_FAQ_collection")
results = collection.query(
    query_texts=[query],
    n_results=3,
)

for i, m in enumerate(results["metadatas"][0]):
    print(f"Result {i+1}:")
    print("Distance:", results["distances"][0][i])
    print("Q:", m["question"])
    print("A:", m["answer"])
    print("-" * 50)

"""#### Generating Synthetic Data"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

def generate_sample_data(random_state=42):
    """Generate enriched sample data for all tables with 50 first/last names"""
    random.seed(random_state)
    np.random.seed(random_state)
    # Define 50 first and last names
    first_names = [
        "John", "Jane", "Robert", "Maria", "David", "Lisa", "Michael", "Sarah", "James", "Emily",
        "William", "Emma", "Joseph", "Olivia", "Charles", "Ava", "Thomas", "Isabella", "Daniel", "Mia",
        "Matthew", "Sophia", "Anthony", "Charlotte", "Christopher", "Amelia", "Andrew", "Harper",
        "Joshua", "Evelyn", "Ryan", "Abigail", "Brandon", "Ella", "Justin", "Scarlett", "Tyler", "Grace",
        "Alexander", "Chloe", "Kevin", "Victoria", "Jason", "Lily", "Brian", "Hannah", "Eric", "Aria",
        "Kyle", "Zoey"
    ]

    last_names = [
        "Smith", "Johnson", "Williams", "Brown", "Jones", "Garcia", "Miller", "Davis", "Rodriguez", "Martinez",
        "Hernandez", "Lopez", "Gonzalez", "Wilson", "Anderson", "Thomas", "Taylor", "Moore", "Jackson", "Martin",
        "Lee", "Perez", "Thompson", "White", "Harris", "Sanchez", "Clark", "Ramirez", "Lewis", "Robinson",
        "Walker", "Young", "Allen", "King", "Wright", "Scott", "Torres", "Nguyen", "Hill", "Flores",
        "Green", "Adams", "Nelson", "Baker", "Hall", "Rivera", "Campbell", "Mitchell", "Carter", "Roberts"
    ]

    # Generate customers (1000 random name combinations)
    customers = pd.DataFrame({
        'customer_id': [f'CUST{str(i).zfill(5)}' for i in range(1, 1001)],
        'first_name': [random.choice(first_names) for _ in range(1000)],
        'last_name': [random.choice(last_names) for _ in range(1000)],
        'email': [f'user{i}@example.com' for i in range(1, 1001)],
        'phone': [f'555-{str(random.randint(100,999)).zfill(3)}-{str(random.randint(1000,9999)).zfill(4)}' for _ in range(1000)],
        'date_of_birth': [datetime(1980, 1, 1) + timedelta(days=random.randint(0, 10000)) for _ in range(1000)],
        'state': [random.choice(['CA', 'NY', 'TX', 'FL', 'IL', 'PA', 'OH', 'GA']) for _ in range(1000)]
    })

    # Policies
    policies = pd.DataFrame({
        'policy_number': [f'POL{str(i).zfill(6)}' for i in range(1, 1501)],
        'customer_id': [f'CUST{str(random.randint(1, 1000)).zfill(5)}' for _ in range(1500)],
        'policy_type': [random.choice(['auto', 'home', 'life']) for _ in range(1500)],
        'start_date': [datetime(2023, 1, 1) + timedelta(days=random.randint(0, 365)) for _ in range(1500)],
        'premium_amount': [round(random.uniform(50, 500), 2) for _ in range(1500)],
        'billing_frequency': [random.choice(['monthly', 'quarterly', 'annual']) for _ in range(1500)],
        'status': [random.choice(['active', 'active', 'active', 'cancelled']) for _ in range(1500)]
    })

    # Auto Policy Details (subset)
    auto_policies = policies[policies['policy_type'] == 'auto'].copy()
    auto_policy_details = pd.DataFrame({
        'policy_number': auto_policies['policy_number'],
        'vehicle_vin': [f'VIN{random.randint(10000000000000000, 99999999999999999)}' for _ in range(len(auto_policies))],
        'vehicle_make': [random.choice(['Toyota', 'Honda', 'Ford', 'Chevrolet', 'Nissan']) for _ in range(len(auto_policies))],
        'vehicle_model': [random.choice(['Camry', 'Civic', 'F-150', 'Malibu', 'Altima']) for _ in range(len(auto_policies))],
        'vehicle_year': [random.randint(2015, 2023) for _ in range(len(auto_policies))],
        'liability_limit': [random.choice([50000, 100000, 300000]) for _ in range(len(auto_policies))],
        'collision_deductible': [random.choice([250, 500, 1000]) for _ in range(len(auto_policies))],
        'comprehensive_deductible': [random.choice([250, 500, 1000]) for _ in range(len(auto_policies))],
        'uninsured_motorist': [random.choice([0, 1]) for _ in range(len(auto_policies))],
        'rental_car_coverage': [random.choice([0, 1]) for _ in range(len(auto_policies))]
    })


    # Billing
    billing = pd.DataFrame({
        'bill_id': [f'BILL{str(i).zfill(6)}' for i in range(1, 5001)],
        'policy_number': [random.choice(policies['policy_number']) for _ in range(5000)],
        'billing_date': [datetime(2024, 1, 1) + timedelta(days=random.randint(0, 90)) for _ in range(5000)],
        'due_date': [datetime(2024, 1, 15) + timedelta(days=random.randint(0, 90)) for _ in range(5000)],
        'amount_due': [round(random.uniform(100, 1000), 2) for _ in range(5000)],
        'status': [random.choice(['paid', 'pending', 'overdue']) for _ in range(5000)]
    })

    # Payments
    payments = pd.DataFrame({
        'payment_id': [f'PAY{str(i).zfill(6)}' for i in range(1, 4001)],
        'bill_id': [random.choice(billing['bill_id']) for _ in range(4000)],
        'payment_date': [datetime(2024, 1, 1) + timedelta(days=random.randint(0, 90)) for _ in range(4000)],
        'amount': [round(random.uniform(50, 500), 2) for _ in range(4000)],
        'payment_method': [random.choice(['credit_card', 'debit_card', 'bank_transfer']) for _ in range(4000)],
        'transaction_id': [f'TXN{random.randint(100000,999999)}' for _ in range(4000)],
        'status': [random.choice(['completed', 'pending', 'failed']) for _ in range(4000)]
    })

    # Claims
    claims = pd.DataFrame({
        'claim_id': [f'CLM{str(i).zfill(6)}' for i in range(1, 301)],
        'policy_number': [random.choice(policies['policy_number']) for _ in range(300)],
        'claim_date': [datetime(2024, 1, 1) + timedelta(days=random.randint(0, 90)) for _ in range(300)],
        'incident_type': [random.choice(['collision', 'theft', 'property_damage', 'medical', 'liability']) for _ in range(300)],
        'estimated_loss': [round(random.uniform(500, 20000), 2) for _ in range(300)],
        'status': [random.choice(['submitted', 'under_review', 'approved', 'paid', 'denied']) for _ in range(300)]
    })



    return {
        'customers': customers,
        'policies': policies,
        'auto_policy_details': auto_policy_details,
        'billing': billing,
        'payments': payments,
        'claims': claims,
    }


sample_data = generate_sample_data()

print("Customers Data Shape:", sample_data["customers"].shape)
sample_data["customers"].head()

print("billing Data Shape:", sample_data["billing"].shape)
sample_data["billing"].head()

print("payments Data Shape:", sample_data["payments"].shape)
sample_data["payments"].head()

print("claims Data Shape:", sample_data["claims"].shape)
sample_data["claims"].head()

print("policies Data Shape:", sample_data["policies"].shape)
sample_data["policies"].head()

print("auto policies Data Shape:", sample_data["auto_policy_details"].shape)
sample_data["auto_policy_details"].head()

"""#### Uploading the Data to SQLite DB"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
import json


def connect_db(db_path='insurance_support.db'):
    """Connect to SQLite database"""
    return sqlite3.connect(db_path)


def drop_and_create_tables(conn):
    """Drop existing tables and recreate the schema"""
    cursor = conn.cursor()

    # Drop tables if exist
    cursor.executescript("""
        DROP TABLE IF EXISTS claims;
        DROP TABLE IF EXISTS payments;
        DROP TABLE IF EXISTS billing;
        DROP TABLE IF EXISTS auto_policy_details;
        DROP TABLE IF EXISTS policies;
        DROP TABLE IF EXISTS customers;
    """)

    # Create tables
    cursor.executescript("""
        CREATE TABLE customers (
            customer_id VARCHAR(20) PRIMARY KEY,
            first_name VARCHAR(50),
            last_name VARCHAR(50),
            email VARCHAR(100),
            phone VARCHAR(20),
            date_of_birth DATE,
            state VARCHAR(20)
        );

        CREATE TABLE policies (
            policy_number VARCHAR(20) PRIMARY KEY,
            customer_id VARCHAR(20),
            policy_type VARCHAR(50),
            start_date DATE,
            premium_amount DECIMAL(10,2),
            billing_frequency VARCHAR(20),
            status VARCHAR(20),
            FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
        );

        CREATE TABLE auto_policy_details (
            policy_number VARCHAR(20) PRIMARY KEY,
            vehicle_vin VARCHAR(50),
            vehicle_make VARCHAR(50),
            vehicle_model VARCHAR(50),
            vehicle_year INTEGER,
            liability_limit DECIMAL(10,2),
            collision_deductible DECIMAL(10,2),
            comprehensive_deductible DECIMAL(10,2),
            uninsured_motorist BOOLEAN,
            rental_car_coverage BOOLEAN,
            FOREIGN KEY (policy_number) REFERENCES policies(policy_number)
        );

        CREATE TABLE billing (
            bill_id VARCHAR(20) PRIMARY KEY,
            policy_number VARCHAR(20),
            billing_date DATE,
            due_date DATE,
            amount_due DECIMAL(10,2),
            status VARCHAR(20),
            FOREIGN KEY (policy_number) REFERENCES policies(policy_number)
        );

        CREATE TABLE payments (
            payment_id VARCHAR(20) PRIMARY KEY,
            bill_id VARCHAR(20),
            payment_date DATE,
            amount DECIMAL(10,2),
            payment_method VARCHAR(50),
            transaction_id VARCHAR(100),
            status VARCHAR(20),
            FOREIGN KEY (bill_id) REFERENCES billing(bill_id)
        );

        CREATE TABLE claims (
            claim_id VARCHAR(20) PRIMARY KEY,
            policy_number VARCHAR(20),
            claim_date DATE,
            incident_type VARCHAR(100),
            estimated_loss DECIMAL(10,2),
            status VARCHAR(20),
            FOREIGN KEY (policy_number) REFERENCES policies(policy_number)
        );

    """)

    conn.commit()

def insert_data(conn, data):
    """Insert all DataFrames into SQLite"""
    for table, df in data.items():
        df.to_sql(table, conn, if_exists='append', index=False)
    conn.commit()


def setup_insurance_database(data):
    """Main function to create and populate the insurance database"""
    conn = connect_db()
    drop_and_create_tables(conn)
    insert_data(conn, data)
    conn.close()
    print("‚úÖ Database created successfully with enriched synthetic data!")



setup_insurance_database(sample_data)

# test the SQL connection
conn = connect_db()
query = "SELECT * FROM customers LIMIT 5;"
df_sql = pd.read_sql_query(query, conn)
conn.close()
df_sql

import json
from typing import List, Dict, Any, Optional

from langchain_groq import ChatGroq
from langchain_core.messages import (
    SystemMessage,
    AIMessage,
    ToolMessage
)

# Initialize Groq LLM
llm = ChatGroq(
    api_key='****',
    model='qwen/qwen3-32b',
    temperature=0
)

def run_llm(
    prompt: str,
    tools: Optional[List[Dict]] = None,
    tool_functions: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Run an LLM request using langchain_groq with optional tool calling.
    """

    messages = [SystemMessage(content=prompt)]

    # Step 1: Bind tools (if provided)
    if tools:
        llm_with_tools = llm.bind_tools(tools)
    else:
        llm_with_tools = llm

    # Step 2: Initial LLM call
    response: AIMessage = llm_with_tools.invoke(messages)
    print("Initial LLM Response:", response)

    # Step 3: If no tool calls, return response directly
    if not response.tool_calls:
        return response.content

    if not tool_functions:
        return response.content + "\n\n‚ö†Ô∏è No tool functions provided."

    # Step 4: Execute tool calls
    tool_messages = []

    for tool_call in response.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call.get("args", {})
        tool_fn = tool_functions.get(tool_name)

        try:
            result = tool_fn(**tool_args) if tool_fn else {
                "error": f"Tool '{tool_name}' not implemented."
            }
        except Exception as e:
            result = {"error": str(e)}

        tool_messages.append(
            ToolMessage(
                tool_call_id=tool_call["id"],
                content=json.dumps(result)
            )
        )

     # Step 5: Second pass ‚Äî feed tool outputs back to LLM
    final_messages = [
        SystemMessage(content=prompt),
        response,
        *tool_messages
    ]

    final_response: AIMessage = llm_with_tools.invoke(final_messages)
    return final_response.content

prompt = "Explain the theory of relativity in simple terms in 30 words."
response = run_llm(prompt)
print(response)

def add_numbers(a: int, b: int):
    return {"result": a + b}

tools = [{
    "type": "function",
    "function": {
        "name": "add_numbers",
        "description": "Add two numbers together",
        "parameters": {
            "type": "object",
            "properties": {
                "a": {"type": "integer"},
                "b": {"type": "integer"}
            },
            "required": ["a", "b"]
        },
    }
}]

prompt = "Please add 3 and 5 using the available tool."
response = run_llm(prompt, tools=tools, tool_functions={"add_numbers": add_numbers})
print(response)

prompt = "Explain the theory of relativity in simple terms in 30 words."
response = run_llm(prompt, tools=tools, tool_functions={"add_numbers": add_numbers})
print(response)

"""#### Defining the Tools"""

# =====================================================
#  Define the TOOL functions (these run locally)
# =====================================================

import logging
from datetime import datetime

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('insurance_agent.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def ask_user(question: str, missing_info: str = ""):
    """Ask the user for input and return the response."""
    logger.info(f"üó£Ô∏è Asking user for input: {question}")
    if missing_info:
        print(f"---USER INPUT REQUIRED---\nMissing information: {missing_info}")
    else:
        print(f"---USER INPUT REQUIRED---")

    answer = input(f"{question}: ")
    return {"context": answer, "source": "User Input"}

def get_policy_details(policy_number: str) -> Dict[str, Any]:
    """Fetch a customer's policy details by policy number"""
    logger.info(f"üîç Fetching policy details for: {policy_number}")
    conn = sqlite3.connect('insurance_support.db')
    cursor = conn.cursor()
    cursor.execute("""
        SELECT p.*, c.first_name, c.last_name
        FROM policies p
        JOIN customers c ON p.customer_id = c.customer_id
        WHERE p.policy_number = ?
    """, (policy_number,))
    result = cursor.fetchone()
    conn.close()
    if result:
        logger.info(f"‚úÖ Policy found: {policy_number}")
        columns = [desc[0] for desc in cursor.description]
        return dict(zip(columns, result))
    logger.warning(f"‚ùå Policy not found: {policy_number}")
    return {"error": "Policy not found"}

def get_claim_status(claim_id: str = None, policy_number: str = None) -> Dict[str, Any]:
    """Get claim status and details"""
    logger.info(f"üîç Fetching claim status - Claim ID: {claim_id}, Policy: {policy_number}")
    conn = sqlite3.connect('insurance_support.db')
    cursor = conn.cursor()
    if claim_id:
        cursor.execute("""
            SELECT c.*, p.policy_type
            FROM claims c
            JOIN policies p ON c.policy_number = p.policy_number
            WHERE c.claim_id = ?
        """, (claim_id,))
    elif policy_number:
        cursor.execute("""
            SELECT c.*, p.policy_type
            FROM claims c
            JOIN policies p ON c.policy_number = p.policy_number
            WHERE c.policy_number = ?
            ORDER BY c.claim_date DESC LIMIT 3
        """, (policy_number,))
    result = cursor.fetchall()
    conn.close()
    if result:
        logger.info(f"‚úÖ Found {len(result)} claim(s)")
        columns = [desc[0] for desc in cursor.description]
        return [dict(zip(columns, row)) for row in result]
    logger.warning("‚ùå No claims found")
    return {"error": "Claim not found"}

def get_billing_info(policy_number: str = None, customer_id: str = None) -> Dict[str, Any]:
    """Get billing information including current balance and due dates"""
    logger.info(f"üîç Fetching billing info - Policy: {policy_number}, Customer: {customer_id}")
    conn = sqlite3.connect('insurance_support.db')
    cursor = conn.cursor()
    if policy_number:
        cursor.execute("""
            SELECT b.*, p.premium_amount, p.billing_frequency
            FROM billing b
            JOIN policies p ON b.policy_number = p.policy_number
            WHERE b.policy_number = ? AND b.status = 'pending'
            ORDER BY b.due_date DESC LIMIT 1
        """, (policy_number,))
    elif customer_id:
        cursor.execute("""
            SELECT b.*, p.premium_amount, p.billing_frequency
            FROM billing b
            JOIN policies p ON b.policy_number = p.policy_number
            WHERE p.customer_id = ? AND b.status = 'pending'
            ORDER BY b.due_date DESC LIMIT 1
        """, (customer_id,))
    result = cursor.fetchone()
    conn.close()
    if result:
        logger.info("‚úÖ Billing info found")
        columns = [desc[0] for desc in cursor.description]
        return dict(zip(columns, result))
    logger.warning("‚ùå Billing info not found")
    return {"error": "Billing information not found"}

def get_payment_history(policy_number: str) -> List[Dict[str, Any]]:
    """Get payment history for a policy"""
    logger.info(f"üîç Fetching payment history for policy: {policy_number}")
    conn = sqlite3.connect('insurance_support.db')
    cursor = conn.cursor()

    cursor.execute("""
        SELECT p.payment_date, p.amount, p.status, p.payment_method
        FROM payments p
        JOIN billing b ON p.bill_id = b.bill_id
        WHERE b.policy_number = ?
        ORDER BY p.payment_date DESC LIMIT 10
    """, (policy_number,))

    results = cursor.fetchall()
    conn.close()

    if results:
        logger.info(f"‚úÖ Found {len(results)} payment records")
        columns = [desc[0] for desc in cursor.description]
        return [dict(zip(columns, row)) for row in results]
    logger.warning("‚ùå No payment history found")
    return []

def get_auto_policy_details(policy_number: str) -> Dict[str, Any]:
    """Get auto-specific policy details including vehicle info and deductibles"""
    logger.info(f"üîç Fetching auto policy details for: {policy_number}")
    conn = sqlite3.connect('insurance_support.db')
    cursor = conn.cursor()

    cursor.execute("""
        SELECT apd.*, p.policy_type, p.premium_amount
        FROM auto_policy_details apd
        JOIN policies p ON apd.policy_number = p.policy_number
        WHERE apd.policy_number = ?
    """, (policy_number,))

    result = cursor.fetchone()
    conn.close()

    if result:
        logger.info("‚úÖ Auto policy details found")
        columns = [desc[0] for desc in cursor.description]
        return dict(zip(columns, result))
    logger.warning("‚ùå Auto policy details not found")
    return {"error": "Auto policy details not found"}

"""#### Defining the Prompts"""

SUPERVISOR_PROMPT = """
You are the SUPERVISOR AGENT managing a team of insurance support specialists.

Your role:
1. Go throught the conversatio history and understand the current requirement.
2. Understand the user's intent and context.
3. Evaluate available information and decide if clarification is needed.
4. Route to the appropriate specialist agent.
5. End conversation when the task is complete.

AVAILABLE INFORMATION:
- Conversation History: {conversation_history}

CRITICAL RULES:
- If policy number is already available, DO NOT ask for it again
- If customer ID is already available, DO NOT ask for it again
- Only use ask_user tool if ESSENTIAL information is missing. Keep the clarification questions minimal (within 15 words) and specific.
- Route directly to appropriate agent if you have sufficient information
- Check the conversation history carefully - policy numbers or customer IDs mentioned earlier in the conversation should be considered available
- If the user just provided information in response to your clarification question, that information is NOW available and should not be asked for again

Specialist agents:
- policy_agent ‚Üí policy details, coverage, endorsements
- billing_agent ‚Üí billing, payments, premium questions
- claims_agent ‚Üí claim filing, tracking, settlements
- human_escalation_agent ‚Üí for complex cases
- general_help_agent ‚Üí for general questions

CLARIFICICATION QUESTION GUIDELINES:
1. Keep questions concise (<=15 words)
2. Ask only for ESSENTIAL missing info (policy number, customer ID, claim ID)

EVALUATION INSTRUCTIONS:
- Review the conversation history thoroughly.
- Agents answers are also part of the conversation history.
- If agents ask for more information, use ask_user tool to get it from the user.
- Evaluate the anwer of the agent carefully to see if the user's question is fully answered.
- If user's question is fully answered, route to 'end'.

DECISION GUIDELINES:
1. Policy/coverage questions ‚Üí policy_agent
2. Billing/payment questions ‚Üí billing_agent
3. Claims questions ‚Üí claims_agent
4. General questions (example: In general, what does life insurance cover?) ‚Üí general_help_agent
5. Complete + answered ‚Üí end

TASK GENERATION GUIDELINES:
1. If routing to a specialist, summarize the user's main request.
2. Keep the policy number, customer ID, claim ID (if applicable and available) in Task also.

Respond in JSON:
{{
  "next_agent": "<agent_name or 'end'>",
  "task": "<concise task description>",
  "justification": "<why this decision>"
}}

Only use ask_user tool if absolutely necessary.
"""





POLICY_AGENT_PROMPT = """
You are a **Policy Specialist Agent** for an insurance company.

Assigned Task:
{task}

Responsibilities:
1. Policy details, coverage, and deductibles
2. Vehicle info and auto policy specifics
3. Endorsements and policy updates

Tools:
- get_policy_details
- get_auto_policy_details

Context:
- Policy Number: {policy_number}
- Customer ID: {customer_id}
- Conversation History: {conversation_history}

Instructions:
- Use tools to retrieve information as needed.
- Ask politely for missing details.
- Keep responses professional and clear.
"""

BILLING_AGENT_PROMPT = """
You are a **Billing Specialist Agent**.

Assigned Task:
{task}

Responsibilities:
1. Billing statements, payments, and invoices
2. Premiums, due dates, and payment history

Instructions:
- Use tools to retrieve billing and payment information.
- Ask politely for any missing details.
- Just answer the questions that are asked. Don't provide extra information.
- If you think the question is answered, don't ask for more information. Just retrun with the specific answer.

Tools:
- get_billing_info
- get_payment_history

Context:
- Conversation History: {conversation_history}
"""

CLAIMS_AGENT_PROMPT = """
You are a **Claims Specialist Agent**.

Assigned Task:
{task}

Responsibilities:
1. Retrieve or update claim status
2. Help file new claims
3. Explain claim process and settlements

Tools:
- get_claim_status

Context:
- Policy Number: {policy_number}
- Claim ID: {claim_id}
- Conversation History: {conversation_history}
"""

GENERAL_HELP_PROMPT = """
You are a **General Help Agent** for insurance customers.

Assigned Task:
{task}

Goal:
Answer FAQs and explain insurance topics in simple, clear, and accurate language.

Context:
- Conversation History: {conversation_history}

Retrieved FAQs from the knowledge base:
{faq_context}

Instructions:
1. Review the retrieved FAQs carefully before answering.
2. If one or more FAQs directly answer the question, use them to construct your response.
3. If the FAQs are related but not exact, summarize the most relevant information.
4. If no relevant FAQs are found, politely inform the user and provide general guidance.
5. Keep responses clear, concise, and written for a non-technical audience.
6. Do not fabricate details beyond what‚Äôs supported by the FAQs or obvious domain knowledge.
7. End by offering further help (e.g., ‚ÄúWould you like to know more about this topic?‚Äù).

Now provide the best possible answer for the user‚Äôs question.
"""

HUMAN_ESCALATION_PROMPT = """
You are handling a **Customer Escalation**.

Assigned Task:
{task}


Conversation History: {conversation_history}

Respond empathetically, acknowledge the request for a human, and confirm that a human representative will join shortly.
Don't attempt to answer any questions or provide information yourself.
Don't ask any further questions. Just acknowledge the escalation request.
"""


FINAL_ANSWER_PROMPT = """
    The user asked: "{user_query}"

    The specialist agent provided this detailed response:
    {specialist_response}

    Your task: Create a FINAL, CLEAN response that:
    1. Directly answers the user's original question in a friendly tone
    2. Includes only the most relevant information (remove technical details)
    3. Is concise and easy to understand
    4. Ends with a polite closing

    Important: Do NOT include any internal instructions, tool calls, or technical details.
    Just provide the final answer that the user should see.

    Final response:
    """

"""#### Setting up the agents"""

def supervisor_agent(state):
    print("---SUPERVISOR AGENT---")
    # Increment iteration counter
    n_iter = state.get("n_iteration", 0) + 1
    state["n_iteration"] = n_iter
    print(f"üî¢ Supervisor iteration: {n_iter}")

    # Force end if iteration limit reached
    # Escalate to human support if iteration limit reached
    if n_iter >= 10: # Increased iteration limit from 3 to 10
        print("‚ö†Ô∏è Maximum supervisor iterations reached ‚Äî escalating to human agent")
        updated_history = (
            state.get("conversation_history", "")
            + "\nAssistant: It seems this issue requires human review. Escalating to a human support specialist."
        )
        return {
            "escalate_to_human": True,
            "conversation_history": updated_history,
            "next_agent": "human_escalation_agent",
            "n_iteration": n_iter
        }

    # Check if we're coming from a clarification
    if state.get("needs_clarification", False):
        user_clarification = state.get("user_clarification", "")
        print(f"üîÑ Processing user clarification: {user_clarification}")

        # Update conversation history with the clarification exchange
        clarification_question = state.get("clarification_question", "")
        updated_conversation = state.get("conversation_history", "") + f"\nAssistant: {clarification_question}\nUser: {user_clarification}"

        # Update state to clear clarification flags and update history
        updated_state = state.copy()
        updated_state["needs_clarification"] = False
        updated_state["conversation_history"] = updated_conversation

        # Clear clarification fields
        if "clarification_question" in updated_state:
            del updated_state["clarification_question"]
        if "user_clarification" in updated_state:
            del updated_state["user_clarification"]

        return updated_state

    user_query = state["user_input"]
    conversation_history = state.get("conversation_history", "")


    print(f"User Query: {user_query}")
    print(f"Conversation History: {conversation_history}")


    # Include the ENTIRE conversation history in the prompt
    full_context = f"Full Conversation:\n{conversation_history}"


    prompt = SUPERVISOR_PROMPT.format(
        conversation_history=full_context,
    )

    tools = [
        {
            "type": "function",
            "function": {
                "name": "ask_user",
                "description": "Ask the user for clarification or additional information when their query is unclear or missing important details. ONLY use this if essential information like policy number or customer ID is missing.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "question": {
                            "type": "string",
                            "description": "The specific question to ask the user for clarification"
                        },
                        "missing_info": {
                            "type": "string",
                            "description": "What specific information is missing or needs clarification"
                        }
                    },
                    "required": ["question", "missing_info"]
                }
            }
        }
    ]

    print("ü§ñ Calling LLM for supervisor decision...")

    messages = [SystemMessage(content=prompt)]
    llm_with_tools = llm.bind_tools(tools) # Bind tools to LLM for the supervisor's call
    response: AIMessage = llm_with_tools.invoke(messages) # response is AIMessage

    # Check if supervisor wants to ask user for clarification
    if response.tool_calls: # Check tool_calls attribute of AIMessage
        print("üõ†Ô∏è Supervisor requesting user clarification")
        for tool_call in response.tool_calls: # Iterate over the tool calls
            # LangChain tool_calls are lists of dicts
            if tool_call["name"] == "ask_user":
                args = tool_call["args"] # Arguments are already a dict
                question = args.get("question", "Can you please provide more details?")
                missing_info = args.get("missing_info", "additional information")

                print(f"‚ùì Asking user: {question}")
                user_response_data = ask_user(question, missing_info)
                user_response = user_response_data["context"]

                print(f"‚úÖ User response: {user_response}")

                updated_history = conversation_history + f"\nAssistant: {question}"
                updated_history = updated_history + f"\nUser: {user_response}"

                return {
                    "needs_clarification": True,
                    "clarification_question": question,
                    "user_clarification": user_response,
                    "conversation_history": updated_history
                }
        # If there were tool_calls but none were 'ask_user', fall through to normal content processing
        message_content = response.content
    else:
        # No tool calls, proceed with normal supervisor decision
        message_content = response.content

    try:
        parsed = json.loads(message_content)
        print("‚úÖ Supervisor output parsed successfully")
    except json.JSONDecodeError:
        print(f"‚ùå Supervisor output invalid JSON: {message_content}, using fallback")
        parsed = {}

    next_agent = parsed.get("next_agent", "general_help_agent")
    task = parsed.get("task", "Assist the user with their query.")
    justification = parsed.get("justification", "")

    print(f"---SUPERVISOR DECISION: {next_agent}---")
    print(f"Task: {task}")
    print(f"Reason: {justification}")

    # Update conversation history with the current exchange
    updated_conversation = conversation_history + f"\nAssistant: Routing to {next_agent} for: {task}"


    print(f"‚û°Ô∏è Routing to: {next_agent}")
    return {
        "next_agent": next_agent,
        "task": task,
        "justification": justification,
        "conversation_history": updated_conversation,
        "n_iteration": n_iter
    }

def claims_agent_node(state):
    logger.info("üè• Claims agent started")
    logger.debug(f"Claims agent state: { {k: v for k, v in state.items() if k != 'messages'} }")

    prompt = CLAIMS_AGENT_PROMPT.format(
        task=state.get("task"),
        policy_number=state.get("policy_number", "Not provided"),
        claim_id=state.get("claim_id", "Not provided"),
        conversation_history=state.get("conversation_history", "")
    )

    tools = [
        {"type": "function", "function": {
            "name": "get_claim_status",
            "description": "Retrieve claim details",
            "parameters": {"type": "object", "properties": {"claim_id": {"type": "string"}, "policy_number": {"type": "string"}}}
        }}
    ]

    result = run_llm(prompt, tools, {"get_claim_status": get_claim_status})

    logger.info("‚úÖ Claims agent completed")
    return {"messages": [("assistant", result)]}

def final_answer_agent(state):
    """Generate a clean final summary before ending the conversation"""
    print("---FINAL ANSWER AGENT---")
    logger.info("üéØ Final answer agent started")

    user_query = state["user_input"]
    conversation_history = state.get("conversation_history", "")

    # Extract the most recent specialist response
    recent_responses = []
    for msg in reversed(state.get("messages", [])):
        if hasattr(msg, 'content') and "clarification" not in msg.content.lower():
            recent_responses.append(msg.content)
            if len(recent_responses) >= 2:
                break

    specialist_response = recent_responses[0] if recent_responses else "No response available"

    prompt = FINAL_ANSWER_PROMPT.format(

        specialist_response=specialist_response,
        user_query=user_query,
    )

    print("ü§ñ Generating final summary...")
    messages = [SystemMessage(content=prompt)]
    response = llm.invoke(messages)
    final_answer = response.content

    print(f"‚úÖ Final answer: {final_answer}")

    # Replace all previous messages with just the final answer
    clean_messages = [("assistant", final_answer)]

    state["final_answer"] = final_answer
    state["end_conversation"] = True
    state["conversation_history"] = conversation_history + f"\nAssistant: {final_answer}"
    state["messages"] = clean_messages

    return state


def policy_agent_node(state):
    print("---POLICY AGENT---")
    logger.info("üìÑ Policy agent started")
    logger.debug(f"Policy agent state: { {k: v for k, v in state.items() if k != 'messages'} }")

    prompt = POLICY_AGENT_PROMPT.format(
        task=state.get("task"),
        policy_number=state.get("policy_number", "Not provided"),
        customer_id=state.get("customer_id", "Not provided"),
        conversation_history=state.get("conversation_history", "")
    )

    tools = [
        {"type": "function", "function": {
            "name": "get_policy_details",
            "description": "Fetch policy info by policy number",
            "parameters": {"type": "object", "properties": {"policy_number": {"type": "string"}}}
        }},
        {"type": "function", "function": {
            "name": "get_auto_policy_details",
            "description": "Get auto policy details",
            "parameters": {"type": "object", "properties": {"policy_number": {"type": "string"}}}
        }}
    ]

    print("üîÑ Processing policy request...")
    result = run_llm(prompt, tools, {
        "get_policy_details": get_policy_details,
        "get_auto_policy_details": get_auto_policy_details
    })

    print("‚úÖ Policy agent completed")
    return {"messages": [("assistant", result)]}

def billing_agent_node(state):
    print("---BILLING AGENT---")
    print("TASK: ", state.get("task"))
    print("USER QUERY: ", state.get("user_input"))
    print("CONVERSATION HISTORY: ", state.get("conversation_history", ""))


    prompt = BILLING_AGENT_PROMPT.format(
        task=state.get("task"),
        conversation_history=state.get("conversation_history", "")
    )

    tools = [
        {"type": "function", "function": {
            "name": "get_billing_info",
            "description": "Retrieve billing information",
            "parameters": {"type": "object", "properties": {"policy_number": {"type": "string"}, "customer_id": {"type": "string"}}}
        }},
        {"type": "function", "function": {
            "name": "get_payment_history",
            "description": "Fetch recent payment history",
            "parameters": {"type": "object", "properties": {"policy_number": {"type": "string"}}}
        }}
    ]

    print("üîÑ Processing billing request...")
    result = run_llm(prompt, tools, {
        "get_billing_info": get_billing_info,
        "get_payment_history": get_payment_history
    })

    print("‚úÖ Billing agent completed")

    # Extract and preserve policy number if mentioned in the conversation
    updated_state = {"messages": [("assistant", result)]}

    # If we have a policy number in state, preserve it
    if state.get("policy_number"):
        updated_state["policy_number"] = state["policy_number"]
    if state.get("customer_id"):
        updated_state["customer_id"] = state["customer_id"]

    # Update conversation history
    current_history = state.get("conversation_history", "")
    updated_state["conversation_history"] = current_history + f"\nBilling Agent: {result}"

    return updated_state


def general_help_agent_node(state):
    print("---GENERAL HELP AGENT---")

    user_query = state.get("user_input", "")
    conversation_history = state.get("conversation_history", "")
    task = state.get("task", "General insurance support")

    # Step 1: Retrieve relevant FAQs from the vector DB
    print("üîç Retrieving FAQs...")
    logger.info("üîç Retrieving FAQs from vector database")
    results = collection.query(
        query_texts=[user_query],
        n_results=3,
        include=["metadatas", "documents", "distances"]
    )

    # Step 2: Format retrieved FAQs
    faq_context = ""
    if results and results.get("metadatas") and results["metadatas"][0]:
        print(f"üìö Found {len(results['metadatas'][0])} relevant FAQs")
        for i, meta in enumerate(results["metadatas"][0]):
            q = meta.get("question", "")
            a = meta.get("answer", "")
            score = results["distances"][0][i]
            faq_context += f"FAQ {i+1} (score: {score:.3f})\nQ: {q}\nA: {a}\n\n"
    else:
        print("‚ùå No relevant FAQs found")
        faq_context = "No relevant FAQs were found."

    # Step 3: Format the final prompt
    prompt = GENERAL_HELP_PROMPT.format(
        task=task,
        conversation_history=conversation_history,
        faq_context=faq_context
    )

    print("ü§ñ Calling LLM for general response...")
    final_answer = run_llm(prompt)



    print("‚úÖ General help agent completed")
    updated_state = {
                        "messages": [("assistant", final_answer)],
                        "retrieved_faqs": results.get("metadatas", []),
                    }


    updated_state["conversation_history"] = conversation_history + f"\nGeneral Help Agent: {final_answer}"

    return updated_state

def human_escalation_node(state):
    print("---HUMAN ESCALATION AGENT---")
    logger.warning(f"Escalation triggered - State: { {k: v for k, v in state.items() if k != 'messages'} }")

    prompt = HUMAN_ESCALATION_PROMPT.format(
        task=state.get("task"),
        conversation_history=state.get("conversation_history", "")
    )

    print("ü§ñ Generating escalation response...")
    messages = [SystemMessage(content=prompt)]
    response = llm.invoke(messages)

    print("üö® Conversation escalated to human")
    return {
        "final_answer": response.content,
        "requires_human_escalation": True,
        "escalation_reason": "Customer requested human assistance.",
        "messages": [("assistant", response.content)]
    }

"""#### Setting up Nodes and Edges in LangGraph"""

from langgraph.graph import StateGraph, END


from typing import TypedDict, List, Annotated, Dict, Any, Optional
from langgraph.graph import add_messages
from datetime import datetime

class GraphState(TypedDict):
    # Core conversation tracking
    messages: Annotated[List[Any], add_messages]
    user_input: str
    conversation_history: Optional[str]

    n_iteration: Optional[int]

    # Extracted context & metadata
    user_intent: Optional[str]            # e.g., "query_policy", "billing_issue"
    customer_id: Optional[str]
    policy_number: Optional[str]
    claim_id: Optional[str]

    # Supervisor / routing layer
    next_agent: Optional[str]             # e.g., "policy_agent", "claims_agent", etc.
    task: Optional[str]                   # Current task determined by supervisor
    justification: Optional[str]          # Supervisor reasoning/explanation
    end_conversation: Optional[bool]      # Flag for graceful conversation termination

    # Entity extraction and DB lookups
    extracted_entities: Dict[str, Any]    # Parsed from user input (dates, names, etc.)
    database_lookup_result: Dict[str, Any]

    # Escalation state
    requires_human_escalation: bool
    escalation_reason: Optional[str]

    # Billing-specific fields
    billing_amount: Optional[float]
    payment_method: Optional[str]
    billing_frequency: Optional[str]      # "monthly", "quarterly", "annual"
    invoice_date: Optional[str]

    # System-level metadata
    timestamp: Optional[str]     # Track time of latest user message or state update
    final_answer: Optional[str]


def decide_next_agent(state):
    # Handle clarification case first
    if state.get("needs_clarification"):
        return "supervisor_agent"  # Return to supervisor to process the clarification

    if state.get("end_conversation"):
        return "end"

    if state.get("requires_human_escalation"):
        return "human_escalation_agent"

    return state.get("next_agent", "general_help_agent")


# Update the workflow to include the final_answer_agent
workflow = StateGraph(GraphState)

workflow.add_node("supervisor_agent", supervisor_agent)
workflow.add_node("policy_agent", policy_agent_node)
workflow.add_node("billing_agent", billing_agent_node)
workflow.add_node("claims_agent", claims_agent_node)
workflow.add_node("general_help_agent", general_help_agent_node)
workflow.add_node("human_escalation_agent", human_escalation_node)
workflow.add_node("final_answer_agent", final_answer_agent)  # Add this

workflow.set_entry_point("supervisor_agent")



workflow.add_conditional_edges(
    "supervisor_agent",
    decide_next_agent,
    {
        "supervisor_agent": "supervisor_agent",
        "policy_agent": "policy_agent",
        "billing_agent": "billing_agent",
        "claims_agent": "claims_agent",
        "human_escalation_agent": "human_escalation_agent",
        "general_help_agent": "general_help_agent",
        "end": "final_answer_agent"
    }
)

# Return to Supervisor after each specialist
for node in ["policy_agent", "billing_agent", "claims_agent", "general_help_agent"]:
    workflow.add_edge(node, "supervisor_agent")

# Final answer agent ‚Üí END
workflow.add_edge("final_answer_agent", END)

# Human escalation ‚Üí END
workflow.add_edge("human_escalation_agent", END)

app = workflow.compile()


# === Display the Graph ===
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

"""#### Testing the Queries"""

def run_test_query(query):
    """Test the system with a billing query"""
    initial_state = {
        "n_iteraton":0,
        "messages": [],
        "user_input": query,
        "user_intent": "",
        "claim_id": "",
        "next_agent": "supervisor_agent",
        "extracted_entities": {},
        "database_lookup_result": {},
        "requires_human_escalation": False,
        "escalation_reason": "",
        "billing_amount": None,
        "payment_method": None,
        "billing_frequency": None,
        "invoice_date": None,
        "conversation_history": f"User: {query}",
        "task": "Help user with their query",
        "final_answer": ""
    }

    print(f"\n{'='*50}")
    print(f"QUERY: {query}")
    print(f"\n{'='*50}")

    # Run the graph
    final_state = app.invoke(initial_state)

    # Print the response
    print("\n---FINAL RESPONSE---")
    final_answer = final_state.get("final_answer", "No final answer generated.")
    print(final_answer)


    return final_state

# POL000004 ,
test_query = "What is the premium of my auto insurance policy?"
final_output =  run_test_query(test_query)

test_query = "In general, what does life insurance cover?"
final_output =  run_test_query(test_query)

test_query = "I want to talk to human executive"
final_output =  run_test_query(test_query)

